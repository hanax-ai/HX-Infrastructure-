# LLM-01 Server Infrastructure

**Server**: hx-llm-server-01  
**Hardware**: Dual RTX 4070 Ti GPUs (32GB VRAM)  
**Service**: Ollama v0.11.4 LLM Inference Platform  
**Status**: üü¢ **OPERATIONAL** (Architecture Aligned)  

---

## üéØ Architecture Alignment Complete

‚úÖ **ALIGNED WITH LLM-02**: llm-01 successfully standardized to canonical `/mnt/active_llm_models` path

| **Component** | **Status** | **Details** |
|---------------|------------|-------------|
| **üñ•Ô∏è Hardware** | üü¢ **ACTIVE** | 2x RTX 4070 Ti, 7.3TB storage |
| **üîß Ollama Service** | üü¢ **RUNNING** | v0.11.4, Port 11434 |
| **üîí Security** | üü¢ **HARDENED** | Least-privilege implemented |
| **üìÅ Storage** | üü¢ **ALIGNED** | Canonical `/mnt/active_llm_models` ‚úÖ |
| **üåê API** | üü¢ **RESPONDING** | http://0.0.0.0:11434 |
| **‚öôÔ∏è Service Scripts** | üü¢ **STANDARDIZED** | Global shared scripts active |
| **ü§ñ Models** | üü¢ **PRESERVED** | All 3 production models (18.4GB) |

---

## Storage Mapping (POST-ALIGNMENT)

**NEW: Canonical Architecture**
- **Canonical Path**: `/mnt/active_llm_models` (standardized with llm-02)
- **Real Storage**: `/data/llm_bulk_storage` (7.3TB)
- **Compatibility**: `/opt/hx-infrastructure/llm-01/models/production` ‚Üí `/mnt/active_llm_models`
- **Migration**: ‚úÖ 18.5GB models migrated with zero data loss

**Legacy Mapping (PRE-ALIGNMENT)**
- ~~NVMe Device: /dev/nvme1n1p1~~ (not in use)
- ~~SATA Device: /dev/sda1~~ (now mounted at canonical location)
- ~~Models Mount: /opt/hx-infrastructure/llm-01/models~~ (now symlinked)
- ~~Bulk Storage Mount: /opt/hx-infrastructure/llm-01/data/llm_bulk_storage~~ (now symlinked)

---

## Model Registry (Current Production)

**‚úÖ User-Preferred Models (Preserved Through Alignment)**:
```bash
# Model inventory maintained during architecture alignment
NAME                       ID              SIZE      MODIFIED   
llama3.2:3b                a80c4f17acd5    2.0 GB    2 days ago    
qwen3:1.7b                 8f68893c685c    1.4 GB    2 days ago    
mistral-small3.2:latest    5a408ab55df5    15 GB     2 days ago
```

**Configuration** (`/opt/hx-infrastructure/llm-01/config/ollama/ollama.env`):
```bash
OLLAMA_MODELS="/mnt/active_llm_models"                         # ‚úÖ Canonical path
OLLAMA_MODEL_LLAMA32="llama3.2:3b"
OLLAMA_MODEL_QWEN3="qwen3:1.7b"  
OLLAMA_MODEL_MISTRAL="mistral-small3.2@sha256:5a408ab55df5"
OLLAMA_MODELS_AVAILABLE="llama3.2:3b,qwen3:1.7b,mistral-small3.2@sha256:5a408ab55df5"
```

---

## Services & Monitoring

**Enhanced Service Management**:
- **Global Scripts**: ‚úÖ Standardized service management via `/opt/scripts/service/ollama/`
- **Local Fallback**: ‚úÖ Symlinks from local paths for compatibility
- **Validation**: ‚úÖ Enhanced configuration validation with `validate-model-config.sh`

**Active Monitoring**:
- **GPU Telemetry**: ‚úÖ Every 5 minutes ‚Üí `/llm-01/logs/gpu/nvidia-smi-ping.csv`
- **Nightly Smoke Tests**: ‚úÖ Daily at 00:03 UTC ‚Üí `/opt/hx-infrastructure/logs/services/ollama/perf/nightly-smoke.log`
- **Performance Baseline**: Current first-token latencies:
  - `llama3.2:3b`: 1.64s (fastest)
  - `qwen3:1.7b`: 1.98s  
  - `mistral-small3.2:latest`: 5.44s (largest)

**Service Status**:
- Ollama: ‚úÖ active and responding
- Node Exporter: ‚úÖ `/opt/hx-infrastructure/llm-01/services/metrics/node-exporter`
- GPU Telemetry: ‚úÖ Automated collection active
- Security Audit: ‚úÖ Zero vulnerabilities (hardened)

---

## Architecture Benefits

### **‚úÖ Standardization Achieved**
- **Cross-Node Consistency**: Both llm-01 and llm-02 use identical canonical paths
- **Simplified Operations**: Global service scripts eliminate node-specific variations
- **Unified Storage**: Consistent `/mnt/active_llm_models` across infrastructure

### **‚úÖ Backward Compatibility**
- **Existing Automation**: Continues to work via compatibility symlinks
- **Zero Breaking Changes**: All existing scripts and references preserved
- **Gradual Migration**: Old paths redirect seamlessly to new architecture

### **‚úÖ Enhanced Capabilities**
- **Validation Framework**: Comprehensive model configuration validation
- **Shared Libraries**: Reusable parsing functions in `/lib/model-config.sh`
- **Production Monitoring**: Automated GPU telemetry and nightly validation

---

## Validation Summary

**‚úÖ Last Health Check**: August 14, 2025
- **Hardware**: 2x RTX 4070 Ti operational, CUDA 12.9 ready
- **Service**: Ollama v0.11.4 active on port 11434
- **Models**: All 3 production models accessible and responding
- **Storage**: 7.3TB canonical storage accessible, models at `/mnt/active_llm_models`

**‚úÖ Enhanced Validation Scripts**:
```bash
# Comprehensive model validation
./validate-model-config.sh /opt/hx-infrastructure/llm-01/config/ollama/ollama.env strict
# Result: ‚úÖ All checks pass, 3 models validated

# Model reference extraction testing  
./test-extraction.sh /opt/hx-infrastructure/llm-01/config/ollama/ollama.env
# Result: ‚úÖ All model references extracted correctly

# External connectivity verification (NEW) ‚ú®
./emb-external-verify.sh 192.168.10.30 11434
# Result: ‚úÖ External access validation with latency/semantic testing
```

**‚úÖ Smoke Tests**: 
- **Global Scripts**: `/opt/scripts/tests/smoke/ollama/smoke.sh` ‚Üí ‚úÖ PASS
- **API Functionality**: All 3 models responding correctly
- **Concurrency**: 4 concurrent requests successful

---

*Generated on August 15, 2025*  
*Architecture Status: ‚úÖ **ALIGNED WITH LLM-02 CANONICAL PATHS***  
*Infrastructure Version: llm-01 Post-Alignment with External Verification*  
*Next Review: Ongoing production monitoring*
