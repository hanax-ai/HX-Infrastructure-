# LLM-02 Server Infrastructure

**Server**: hx-llm-server-02  
**Hardware**: Dual RTX 5060 Ti GPUs (32GB VRAM)  
**Service**: Ollama v0.11.4 LLM Inference Platform  
**Status**: ğŸŸ¢ **OPERATIONAL** (Phase 5 Complete)  

---

## Quick Status Overview

| **Component** | **Status** | **Details** |
|---------------|------------|-------------|
| **ğŸ–¥ï¸ Hardware** | ğŸŸ¢ **ACTIVE** | 2x RTX 5060 Ti, 3.5TB storage |
| **ğŸ”§ Ollama Service** | ğŸŸ¢ **RUNNING** | v0.11.4, Port 11434 |
| **ğŸ”’ Security** | ğŸŸ¢ **HARDENED** | Systemd restrictions active |
| **ğŸ“ Storage** | ğŸŸ¢ **READY** | /mnt/active_llm_models (3.5TB) |
| **ğŸŒ API** | ğŸŸ¢ **RESPONDING** | http://0.0.0.0:11434 |
| **âš™ï¸ Service Scripts** | ğŸŸ¢ **DEPLOYED** | 4 scripts: start/stop/restart/status |

---

## Directory Structure

```
llm-02/
â”œâ”€â”€ README.md                    # This file - server overview
â”œâ”€â”€ backups/                     # Backup storage location
â”œâ”€â”€ config/                      # Configuration files
â”‚   â”œâ”€â”€ ollama/
â”‚   â”‚   â””â”€â”€ ollama.env          # Ollama environment config (local)
â”‚   â””â”€â”€ readme/
â”‚       â””â”€â”€ template.md.j2      # README template
â”œâ”€â”€ health/                      # Health monitoring
â”‚   â””â”€â”€ scripts/
â”‚       â””â”€â”€ preflight-check.sh  # System validation script
â”œâ”€â”€ scripts/                     # Operational scripts
â”‚   â”œâ”€â”€ maintenance/            # Installation & maintenance
â”‚   â”‚   â”œâ”€â”€ apply-systemd-override.sh
â”‚   â”‚   â””â”€â”€ install-ollama.sh   # Ollama installation script
â”‚   â”œâ”€â”€ service/               # Service management
â”‚   â”‚   â””â”€â”€ ollama/            # Ollama-specific scripts
â”‚   â””â”€â”€ tests/                 # Testing & validation
â”‚       â””â”€â”€ smoke/             # Smoke tests
â”‚           â””â”€â”€ ollama/        # Ollama smoke tests
â”œâ”€â”€ services/                   # Service configurations
â”‚   â””â”€â”€ metrics/               # Monitoring services
â”‚       â””â”€â”€ node-exporter/     # System metrics
â””â”€â”€ x-Docs/                    # Documentation
    â”œâ”€â”€ code-enhancements.md   # Development improvements log
    â””â”€â”€ deployment-status-tracker.md  # Detailed deployment status
```

---

## Service Management

### **Quick Commands**

```bash
# Check service status
sudo systemctl status ollama

# Test API endpoint
curl -s http://localhost:11434/api/tags

# Check system resources
./health/scripts/preflight-check.sh

# View recent logs
sudo journalctl -u ollama --no-pager -n 20
```

### **Configuration Locations**

- **System Environment**: `/opt/hx-infrastructure/config/ollama/ollama.env`
- **Service Logs**: `/opt/hx-infrastructure/logs/services/ollama/`
- **Model Storage**: `/mnt/active_llm_models/`
- **Systemd Override**: `/etc/systemd/system/ollama.service.d/override.conf`

---

## Hardware Specifications

### **GPU Configuration**
- **GPU 0**: NVIDIA RTX 5060 Ti (15.3 GiB VRAM)
- **GPU 1**: NVIDIA RTX 5060 Ti (15.3 GiB VRAM)
- **Total Compute**: 32GB VRAM available for inference
- **CUDA Version**: 12.9
- **Driver**: 575.64.03

### **Storage**
- **Model Storage**: 3.5TB dedicated LLM model storage
- **Mount Point**: `/mnt/active_llm_models`
- **Usage**: ~2% (45GB used / 3.6TB total)

---

## Security Configuration

### **Systemd Hardening**
- âœ… **NoNewPrivileges**: Prevents privilege escalation
- âœ… **PrivateTmp**: Isolated temporary directories
- âœ… **ProtectSystem**: File system protection (strict mode)
- âœ… **ProtectHome**: User directory protection

### **Environment Security**
- âœ… **Secure Config File**: `/opt/hx-infrastructure/config/ollama/ollama.env` (640 root:root)
- âœ… **Service User**: Dedicated `ollama` user with minimal privileges
- âœ… **Port Binding**: 0.0.0.0:11434 (external access ready)

---

## Current Deployment Status

### âœ… **COMPLETED PHASES**

1. **Phase 1**: Infrastructure Setup - Directory structure and Git integration
2. **Phase 2**: System Validation - Preflight checks and GPU diagnostics  
3. **Phase 3**: Ollama Installation - Service deployment and GPU integration
4. **Phase 4**: Secure Environment Configuration - Production security hardening
5. **Phase 5**: Service Management Scripts - Standardized operations (start/stop/restart/status)

### ğŸ”„ **NEXT PHASE**

**Phase 6**: Testing & Validation
- Create comprehensive smoke test scripts
- Implement model download and inference testing
- Establish performance baselines and monitoring

---

## Troubleshooting

### **Common Checks**

```bash
# Verify GPU availability
nvidia-smi

# Check service health
sudo systemctl status ollama

# Test API connectivity
timeout 10s curl -s http://localhost:11434/api/tags

# Check disk space
df -h /mnt/active_llm_models

# View error logs
sudo journalctl -u ollama --no-pager -n 50 | grep -i error
```

### **Known Issues**
- **GPU Warning**: CUDA driver initialization warning (non-critical, GPUs still detected)
- **API Latency**: First API call may have higher latency (normal cold start behavior)

---

## Documentation Links

- **ğŸ“Š Deployment Status**: [deployment-status-tracker.md](x-Docs/deployment-status-tracker.md)
- **ğŸ”§ Code Enhancements**: [code-enhancements.md](x-Docs/code-enhancements.md)
- **âš¡ Preflight Checks**: [preflight-check.sh](health/scripts/preflight-check.sh)
- **ğŸ“¦ Installation Script**: [install-ollama.sh](scripts/maintenance/install-ollama.sh)

---

## Support & Contact

**Maintainer**: HX-Infrastructure Team  
**Primary Contact**: jarvisr@hana-x.ai  
**Repository**: [HX-Infrastructure](https://github.com/hanax-ai/HX-Infrastructure-)  
**Server Location**: hx-llm-server-02  

---

*Last Updated: August 13, 2025*  
*Infrastructure Version: llm-02 Phase 5*  
*Service Status: Production Ready*
